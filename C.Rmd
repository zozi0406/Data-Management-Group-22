---
title: "C"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xml2)
library(tidyverse)
library(rvest)
library(RSQLite)
library(lubridate)
library(textutils)
```

```{r}
#Root website link
link<-"https://data.food.gov.uk/catalog/datasets/38dd8d6a-5ab1-4f50-b753-ab33288e3200"

#Extract data download links from html page
xmllinks<-read_html(link) %>% html_nodes(".o-dataset-distribution--link") %>% html_attr("href")
#Filter to only .xml files (to remove a link that points to webpage index)
xmllinks<-xmllinks[xmllinks %>% str_detect(".xml$")]
#Filter to English language files (some were duplicates in Welsh)
xmllinks<-xmllinks[xmllinks %>% str_detect("en-GB")]

#Create directory to store XMLs.
dir.create("xmldata",showWarnings = F)

#Create empty list to store dataframes
xmldatalist=list()

#Create SQLite connection
con<-RSQLite::dbConnect(RSQLite::SQLite(),"ratings.db")

#Around 10 minutes runtime excl. download
if (!RSQLite::dbExistsTable(con,"fact_Rating")&!file.exists("C_output.rds")) {
  for (i in 1:length(xmllinks)) {
    
    #Get link based on loop variable.
    link <- xmllinks[i]
    
    #Create file path for file to be saved
    filename <- paste0("xmldata/",link %>% str_extract("(?<=/)[^/]+$"))
    
    #Download file if the file does not yet exist.
    if (!file.exists(filename)){
      download.file(link,filename)
    }
    
    #Read xml file and find all EstablishmentDetail nodes.
    nodes <- read_xml(filename) %>% xml_find_all("//EstablishmentDetail")
    
    #Extract ID column to be used as an index later.
    ids<-xml_child(nodes,"FHRSID") %>% xml_text %>% trimws
    
    #Find the amount of fields for every node that are populated
    nodelengths<-xml_length(nodes)
    
    #Find the names of the populated fields for every node
    nodenames<-xml_children(nodes) %>% xml_name
    
    #Find the values of the populated fields for every node
    nodevalues<-xml_children(nodes) %>% xml_text %>% trimws
    
    #Create a long dataframe with node names and values, indexed by repeating the index node-length times.
    #After that, pivot to a wide-format.
    #Finally, add the values of the nested fields (Geocode and Scores) separately, as they would have been merged into one column otherwise.
    df<-tibble(ID=rep(ids,nodelengths),
                   variable=nodenames,
                   values=nodevalues) %>%
      pivot_wider(names_from="variable",values_from="values") %>%
      add_column(Longitude=xml_child(nodes,search="Geocode") %>% xml_child("Longitude") %>% xml_text,
                 Latitude=xml_child(nodes,search="Geocode") %>% xml_child("Latitude") %>% xml_text,
                 Hygiene=xml_child(nodes,search="Scores") %>% xml_child("Hygiene") %>% xml_text,
                 Structural=xml_child(nodes,search="Scores") %>% xml_child("Structural") %>% xml_text,
                 ConfidenceInManagement=xml_child(nodes,search="Scores") %>% xml_child("ConfidenceInManagement") %>% xml_text)
    
    #Remove duplicate and unnecessary columns
    df$Scores<-NULL
    df$Geocode<-NULL
    df$ID<-NULL
    
    #Progress tracker for the parsing process, prints a message every 10 files.
    xmldatalist[[filename]]<-df
    if (i %% 10 == 0){
      paste0(i, " files processed from a total of ", length(xmllinks),".") %>% print()
    }
    
  }
  print("XML parsing finished.")
  
  #Binds data together and detects column types.
  ratingsdata<-do.call(bind_rows,xmldatalist) %>% type_convert()
  
  #Decode HTML encoded characters and remove tags, newlines and tags.
  ratingsdata$RightToReply<-ifelse(is.na(ratingsdata$RightToReply),NA,ratingsdata$RightToReply[!is.na(ratingsdata$RightToReply)] %>%
                                     textutils::HTMLdecode() %>%
                                     str_remove_all("</?p>") %>%
                                     str_remove_all("[\n\t]") %>%
                                     trimws)

  #Convert to dimensional model
  dim_LocalAuthority<-ratingsdata %>% select(LocalAuthorityCode,LocalAuthorityName,LocalAuthorityWebSite,LocalAuthorityEmailAddress) %>% distinct
  dim_Date<-ratingsdata %>% select(RatingDate) %>% mutate(year = lubridate::year(RatingDate),
                                                          month = lubridate::month(RatingDate),
                                                          day = lubridate::day(RatingDate)) %>% distinct()
  dim_BusinessType<-ratingsdata %>% select(BusinessType,BusinessTypeID) %>% distinct()
  dim_RatingTypes<-ratingsdata %>% select(RatingKey,SchemeType,RatingValue) %>% distinct
  fact_Rating <- ratingsdata %>% select(-c(LocalAuthorityName,LocalAuthorityWebSite,LocalAuthorityEmailAddress,BusinessType,RatingValue,SchemeType))
  
  #Write to SQLite database
  RSQLite::dbWriteTable(con,"fact_Rating",fact_Rating)
  RSQLite::dbWriteTable(con,"dim_Date",dim_Date)
  RSQLite::dbWriteTable(con,"dim_LocalAuthority",dim_LocalAuthority)
  RSQLite::dbWriteTable(con,"dim_BusinessType",dim_BusinessType)
  RSQLite::dbWriteTable(con,"dim_RatingTypes",dim_RatingTypes)
  
  saveRDS(ratingsdata,"C_output.rds")
} else {
  ratingsdata<-readRDS("C_output.rds")
} 

#Disconnect from database
RSQLite::dbDisconnect(con)
```


